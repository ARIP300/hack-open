# Chatbot breakthrough in the 2020s? An ethical reflection on the trend of automated consultations…

may not feel adequately connected to their primary physician (Palanica et al. 2019).

Pasquale (2020, p. 57) has reminded us that AI-driven systems, including chatbots, mirror the successes and failures of clinicians. However, machines do not have the human capabilities of prudence and practical wisdom or the flexible, interpretive capacity to correct mistakes and wrong decisions. As a result of self-diagnosis, physicians may have difficulty convincing patients of their potential preliminary, chatbot-derived misdiagnosis. This level of persuasion and negotiation increases the workload of professionals and creates new tensions between patients and physicians. Physicians’ autonomy to diagnose diseases is no end in itself, but patients’ trust in a chatbot about the nature of their disease can impair professionals in their ability to provide appropriate care for patients if they disregard a doctor’s view.

UK health authorities have recommended apps, such as Woebot, for those suffering from depression and anxiety (Jesus 2019). Pasquale (2020, p. 46) pondered, ironically, that cheap mental health apps are a godsend for health systems pressed by austerity cuts, such as Britain’s National Health Service. Unfortunately, according to a study in the journal Evidence Based Mental Health, the true clinical value of most apps was ‘impossible to determine’. To develop social bots, designers leverage the abundance of human–human social media conversations that model, analyse and generate utterances through NLP modules. However, the use of therapy chatbots among vulnerable patients with mental health problems brings many sensitive ethical issues to the fore. For instance, Galitsky’s (2019) experiments with chatbot training showed that people and deep learning chatbots lose their train of thought during conversations, make loose associations between topics (tangentially jumping from one topic to another, apparently at random or on the barest of associations) and give answers to unrelated questions.

It is important to emphasise here that chatbots or more advanced AI-driven systems complement rather than replace medical professionals (Powell 2019). The problem, however, is that new technologies and the inconveniences associated with their use, as well as the increasing amount of data provided within them, are in themselves a new burden on health professionals. Of course, some technologies do prove their worth and gradually facilitate many work processes. However, this is not always the case. Powell (2019) used the example of an early study of an algorithm-based triage tool in primary care. His research showed that physicians lacked trust in the ability of the machine to take clinical risks and worried about issues of governance and accountability, such that the sensitivity of the tool, in terms of the urgency of triage, was consistently set at a threshold that would increase urgent clinical workload rather than reduce it. If there is a great amount of uncertainty in the functioning of a system, Pasquale (2020, p. 56) called for caution because.