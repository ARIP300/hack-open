Furthermore, there are work-related and ethical standards in different fields, which have been developed through centuries or longer. For example, as Pasquale argued (2020, p. 57), in medical fields, science has made medicine and practices more reliable, and ‘medical boards developed standards to protect patients from quacks and charlatans’. Thus, one should be cautious when providing and marketing applications such as chatbots to patients. The application should be in line with up-to-date medical regulations, ethical codes and research data. Pasquale pointed to an Australian study of 82 mobile apps ‘marketed to those suffering from bipolar disorder’, only to find out that ‘the apps were, in general, not in line with practice guidelines or established self-management principles’ (p. 57).

One of the key elements of expertise and its recognition is that patients and others can trust the opinions and decisions offered by the expert/professional. However, in the case of chatbots, ‘the most important factor for explaining trust’ (Nordheim et al. 2019, p. 24) seems to be expertise. People can trust chatbots if they are seen as ‘experts’ (or as possessing expertise of some kind), while expertise itself requires maintaining this trust or trustworthiness. Chatbot users (patients) need to see and experience the bots as ‘providing answers reflecting knowledge, competence, and experience’ (p. 24)—all of which are important to trust. In practice, ‘chatbot expertise’ has to do with, for example, giving a correct answer (provision of accurate and relevant information). The importance of providing correct answers has been found in previous studies (Nordheim et al. 2019, p. 25), which have ‘identified the perceived ability of software agents as a strong predictor of trust’. Conversely, automation errors have a negative effect on trust—‘more so than do similar errors from human experts’ (p. 25). However, the details of experiencing chatbots and their expertise as trustworthy are a complex matter. As Nordheim et al. have pointed out, ‘the answers not only have to be correct, but they also need to adequately fulfil the users’ needs and expectations for a good answer’ (p. 25). Importantly, in addition to human-like answers, the perceived human-likeness of chatbots in general can be considered ‘as a likely predictor of users’ trust in chatbots’ (p. 25).

Based on physician perceptions regarding the use of healthcare chatbots, including their benefits, challenges and risks to patients, Palanica et al. (2019) concluded that the majority of physicians believed that chatbots were unable to effectively care for all patient needs or understand or display human emotion. Because chatbots lack the intelligence to accurately assess patients, they cannot provide detailed clarifications regarding patient assessments, are unable to assess emergency health situations or may indirectly harm patients by not knowing all the personal factors associated with specific patients. In addition, many physicians stated that healthcare chatbots are associated with the risk that patients may self-diagnose too often, that patients may not understand the diagnoses or that patients...